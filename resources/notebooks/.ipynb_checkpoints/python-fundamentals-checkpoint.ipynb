{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='0'></a>\n",
    "\n",
    "# A&C Training Academy\n",
    "## 1st Day - Python and its Essentials\n",
    "\n",
    "In this first module we're going to explore the fundamentals of Python throughout some functional programming exercices, explore some basic operations between `pandas` dataframes, and termine with a walk-through in data visualization by using libraries like `plotly`, `tabulate`, and `panel`. \n",
    "\n",
    ">- `Authors`: Bruno Parra, Bárbara Correia e João Nisa\n",
    ">- `Last Modified`: 14/01/2021\n",
    ">- `Version`: v0.01\n",
    "\n",
    "#### Table of Contents\n",
    "\n",
    "1. [Functional Programming](#1)<br>\n",
    "2. [Pandas Dataframes Basic Operations](#2)<br>\n",
    "    2.1 [Join](#2_1)<br>\n",
    "    2.2 [Group By](#2_2)<br>\n",
    "    2.3 [Filter](#2_3)<br>\n",
    "3. [Data Visualization with `plotly` and `panel`](#3)<br>\n",
    "    3.1 [Pie Charts](#3_1)<br>\n",
    "    3.2 [Heat Maps](#3_2)<br>\n",
    "    3.3 [Time Series Line Plot](#3_3)<br>\n",
    "    3.4 [Tables](#3_4)<br>\n",
    "    3.5 [Panel Dashboards](#3_5)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=2></a>\n",
    "\n",
    "### 2. Pandas Dataframes Basic Operations\n",
    "([Back to Index](#0))<br>\n",
    "\n",
    "In this section you will encounter challenges that allow you to practise the most simple operations you can perform with `pandas` dataframes. For that purpose the minimum requirements must be encountered.\n",
    "\n",
    "#### Requirements\n",
    "\n",
    "Please install the following libraries in the environment that you're using to open this Jupyter notebook. In the terminal of that environment, run the following commands:\n",
    "\n",
    ">- `pip3 install pandas` \n",
    ">- `pip3 install numpy`\n",
    "\n",
    "\n",
    "**Note:** The versions of the libraries above must be the following: <br>\n",
    ">- `pandas v1.2.0` <br>\n",
    ">- `numpy  v1.18.4`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports\n",
    "\n",
    "Specification of all imports, static variables, and configurations required to run the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data upload\n",
    "\n",
    "Specification of the path and import a specific portion of the dataset and the Topology feature as well.\n",
    "\n",
    "**Note:** Don't forget to update the user variable with your own user!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify your user name (TO BE UPDATED!)\n",
    "user = 'jnisa'\n",
    "\n",
    "# path to the dataset\n",
    "base_path = os.path.join(\"C:\\\\\", \"Users\", user, \"Desktop\", \"A&C-Training\", \"training\", \"resources\")\n",
    "\n",
    "# topology file\n",
    "df_tpl = pd.read_csv(os.path.join(base_path, 'notebooks', 'features','Topology.csv'))\n",
    "\n",
    "# output variables\n",
    "ans_1_ = pd.read_parquet(os.path.join(base_path, 'notebooks', 'results', 'df_join_ans_.parquet.gzip'))\n",
    "ans_2_ = pd.read_parquet(os.path.join(base_path, 'notebooks', 'results', 'df_groupby_ans_.parquet.gzip'))\n",
    "ans_3a_ = pd.read_parquet(os.path.join(base_path, 'notebooks', 'results', 'df_filter1_ans_.parquet.gzip'))\n",
    "ans_3b_ = pd.read_parquet(os.path.join(base_path, 'notebooks', 'results', 'df_filter2_ans_.parquet.gzip'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2_1'></a>\n",
    "\n",
    "#### 2.1 Join\n",
    "([Back to Index](#0))\n",
    "\n",
    "Join is one of the most commonly used operations for a Data Engineer to master. \n",
    "\n",
    "In this section we challenge you to merge the dataset partition from **10th May of 2020** with the **Topology**. \n",
    "\n",
    "In the end you must compare the dataframe that you have obtained with the correct one. <br>\n",
    "If you get the `bool` value `True`, in the **Output Validation Section** you are good to go to the next exercise.\n",
    "\n",
    "\n",
    "**Note:** The resultant pandas dataframe must have the name **`df_join_`**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Auxiliar Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from May 10th of 2020\n",
    "partitions_path = os.path.join(base_path, \"data\", \"partitions\", \"2020\", \"05\", \"10\")\n",
    "part_file = \"data_2020-05-10.csv\"\n",
    "\n",
    "# columns required for final df\n",
    "tpl_cols = ['ENODEBID', 'LOCALCELL_ID', 'CELL_NAME', 'DLBANDWIDTH_MHZ', 'DLEARFCN', 'TAC', 'FREQUENCY_TYPE', 'HCS_LAYER']\n",
    "\n",
    "# columns required for the merge\n",
    "cols2_merge_ = tpl_cols[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create dataframe and select correct columns\n",
    "df_day = pd.read_csv(os.path.join(partitions_path, part_file))\n",
    "df_day = df_day.drop([\"Unnamed: 0\"], axis=1)\n",
    "\n",
    "# rename columns\n",
    "df_tpl = df_tpl.rename(columns = {'CELL_ID': 'LOCALCELL_ID'})\n",
    "df_tpl = df_tpl[tpl_cols]\n",
    "\n",
    "# merge topology features\n",
    "df_join_ = pd.merge(df_day, df_tpl, on=cols2_merge_, how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Output validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_join_.equals(ans_1_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Replace nan values for the visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_join_ = df_join_.fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2_2'></a>\n",
    "\n",
    "#### 2.2 Group By\n",
    "([Back to Index](#0))\n",
    "\n",
    "Another commonly used operation in data pipelines is Group By. \n",
    "\n",
    "In this section we challenge you to **groupby the `df_join_` (obtained in the previous section)** per hour and obtain the mean value for the following columns: L_THRP_BITS_DL e L_CHMEAS_PRD_DL_USED_AVG. \n",
    "\n",
    "In the end you must compare the dataframe that you have obtained with the correct one. <br>\n",
    "If you get the `bool` value `True`, in the **Output Validation Section** you are good to go to the next exercise.\n",
    "\n",
    "**Note:** The resultant pandas dataframe must have the name **`df_groupby_`**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Auxiliar Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start coding here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get a column with the hours only \n",
    "df_join_['HOUR'] = df_join_['DATETIME'].apply( lambda x: x.split(' ')[-1])\n",
    "\n",
    "# groupby application\n",
    "df_groupby_ = df_join_.groupby(\"HOUR\").mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Output Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_groupby_.equals(ans_2_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2_3'></a>\n",
    "\n",
    "#### 2.3 Filter\n",
    "([Back to Index](#0))\n",
    "\n",
    "To get specific portions of large datasets, filters by specifics columns and its values can be useful. \n",
    "\n",
    "In this section we want you to **filter the dataset** for the following columns: **L_THRP_BITS_DL** e **L_CHMEAS_PRD_DL_USED_AVG** and values above or equal to 5x10^9 and less than 5, respectively. \n",
    "\n",
    "In the end you must compare the dataframes that you have obtained with the correct one. <br>\n",
    "If you get the `bool` value `True`, in the **Output Validation Section** you are good to go to the next exercise.\n",
    "\n",
    "\n",
    "**Note:** The resultant pandas dataframe must have the name **`df_filter_1_`** and **`df_filter_2_`**, each one for the two filters mentioned above!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Auxiliar Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start coding here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  first we can select only the columsn we want to see\n",
    "df_filter_cols = df_groupby_.filter(items=['ENODEBID', 'LOCALCELL_ID', 'L_THRP_BITS_DL',\n",
    "       'L_CHMEAS_PRB_DL_USED_AVG', 'DLEARFCN', 'TAC'])\n",
    "\n",
    "# then we can choose to filter one column at a time \n",
    "df_filter_1_ = df_filter_cols[df_filter_cols[\"L_THRP_BITS_DL\"] >= 5e9]\n",
    "df_filter_1_ = df_filter_1_[df_filter_1_[\"L_CHMEAS_PRB_DL_USED_AVG\"] < 5]\n",
    "\n",
    "# or both at the same time\n",
    "df_filter_2_ = df_filter_cols[(df_filter_cols.L_THRP_BITS_DL >= 5e9) \n",
    "                        & (df_filter_cols.L_CHMEAS_PRB_DL_USED_AVG < 5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Output Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_filter_1_.equals(ans_3a_)) and (df_filter_2_.equals(ans_3b_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=3></a>\n",
    "\n",
    "### 3. Data Visualization with plotly and panel\n",
    "([Back to Index](#0))<br>\n",
    "\n",
    "In this section you will find some data visualization challenges, for that purpose it is important to gather the minimum requeriments to execute those execises.\n",
    "\n",
    "#### Requirements\n",
    "\n",
    "Please install the following libraries in the environment that you're using to open this Jupyter notebook. In the terminal of that environment, run the following commands:\n",
    "\n",
    ">- `pip3 install plotly`\n",
    ">- `pip3 install panel`\n",
    "\n",
    "If you need documentation regarding the libraries mentioned above, check the following websites:\n",
    ">- `plotly:` https://plotly.com/python/\n",
    ">- `panel :` https://panel.holoviz.org/\n",
    "\n",
    "\n",
    "**Note:** The versions of the libraries above must be the following: <br>\n",
    ">- `plotly v4.14.1` <br>\n",
    ">- `panel  v0.10.2`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports\n",
    "\n",
    "This section should be used to import the libraries needed to make the next exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import panel as pn\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3_1'></a>\n",
    "\n",
    "#### 3.1 Pie Charts\n",
    "([Back to Index](#0))\n",
    "\n",
    "Pie Charts are an import visual when you work with distributions/proportions since each its arc length represents the proportion of the class in the universe that under study.\n",
    "\n",
    "In this section we want to **create three Pie Charts in the same plot**, to obtain representations of the following:\n",
    "\n",
    ">- Distribution of the **Region** (which corresponds to the **TAC** column);\n",
    ">- Distribution of the **Hierarchical Cell Structure** (which corresponds to the **HCS_LAYER** column);\n",
    ">- Distribution of the **Frequency Bands** (which corresponds to the **DLEARFCN** column).\n",
    "\n",
    "In the end you must get the following result:\n",
    "<img src=\"./images/pie-charts-output.png\">\n",
    "\n",
    "**Note:** To execute this exercise please use the dataframe obtained from the join section (**df_join_**)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Auxiliar Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_percentages(df, col):\n",
    "    '''\n",
    "    Function responsible for computing the distribution of each value\n",
    "    '''\n",
    "\n",
    "    ans_ = {} \n",
    "    \n",
    "    for val in list(df[col].unique()):\n",
    "        ans_[val] = int(round(list(df[col]).count(val)/len(list(df[col])) * 100))\n",
    "    \n",
    "    return ans_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pie_charts():\n",
    "    name_1_ = 'TAC Dist.'\n",
    "    name_2_ = 'HCS Dist.'\n",
    "    name_3_ = 'FQ Dist.'\n",
    "\n",
    "    col_1_ = 'TAC'\n",
    "    col_2_ = 'HCS_LAYER'\n",
    "    col_3_ = 'DLEARFCN'\n",
    "\n",
    "    labels_d1_ = list(df_join_[col_1_].unique())\n",
    "    labels_d2_ = list(df_join_[col_2_].unique())\n",
    "    labels_d3_ = list(df_join_[col_3_].unique())\n",
    "\n",
    "    fig = make_subplots(rows=1, cols=3, specs=[[{'type':'domain'}, {'type':'domain'}, {'type':'domain'}]])\n",
    "\n",
    "    # TAC pie-chart distribution \n",
    "    fig.add_trace(go.Pie(labels= list(compute_percentages(df_join_, col_1_).keys()), values= list(compute_percentages(df_join_, col_1_).values()), name=name_1_),\n",
    "                  1, 1)\n",
    "\n",
    "    # HCS pie-chart distribution\n",
    "    fig.add_trace(go.Pie(labels= list(compute_percentages(df_join_, col_2_).keys()), values= list(compute_percentages(df_join_, col_2_).values()), name= name_2_),\n",
    "                  1, 2)\n",
    "\n",
    "    # DLEARFCN pie-chart distribution\n",
    "    fig.add_trace(go.Pie(labels= list(compute_percentages(df_join_, col_3_).keys()), values= list(compute_percentages(df_join_, col_3_).values()), name= name_3_),\n",
    "                  1, 3)\n",
    "\n",
    "    fig.update_layout(title_text= 'Exercise 1 - Pie Charts', width=950)\n",
    "    fig.update(layout_showlegend=False)\n",
    "\n",
    "    return fig\n",
    "    \n",
    "# call the main function\n",
    "pie_charts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3_2'></a>\n",
    "\n",
    "#### 3.2 Heat Maps\n",
    "([Back to Index](#0))\n",
    "\n",
    "Following the theme of the previous section, Heat Maps are also a crucial part when it comes to study proportions and distributions. These can also be obtained by using the `plotly` library.\n",
    "\n",
    "In this section we want to **create a Heat Map** to represent the distribution of the **Frequency Band** (which corresponds to the **DLEARFCN** column).\n",
    "\n",
    "In the end you must get the following result:\n",
    "<img src=\"./images/heat-map-output.png\">\n",
    "\n",
    "**Note:** To execute this exercise please use the dataframe obtained from the join section (**df_join_**)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Auxiliar Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start coding here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def heat_map():\n",
    "    '''\n",
    "    Function responsible for returning a heat map as output\n",
    "    '''\n",
    "    \n",
    "    cols_hm_ = ['L_THRP_BITS_DL', 'FREQUENCY_TYPE', 'DLEARFCN', 'DLBANDWIDTH_MHZ', 'TAC']\n",
    "    \n",
    "    dim = df_join_.groupby(cols_hm_[1:])[cols_hm_[0]].sum().reset_index(name=cols_hm_[0])\n",
    "    dim['Total'] = 'Total'\n",
    "    \n",
    "    fig = px.treemap(dim, path=['Total', *cols_hm_[1:]] ,color=cols_hm_[0], color_continuous_scale='Temps')\n",
    "    fig.update_layout(title_text= 'Exercise 2 - Heat Maps', width=950)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "# call the heat_map function\n",
    "heat_map()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3_3'></a>\n",
    "\n",
    "#### 3.3 Time Series Line Plot \n",
    "([Back to Index](#0))\n",
    "\n",
    "When it comes to perceive variations along time, Time Series Line Plots are always there. \n",
    "\n",
    "In this section we want you to **create a Time Series graphic** to represent the distribution of the **PRB Utilization** and also **Throughput** (which corresponds to the **L_CHMEAS_PRB_DL_USED_AVG** and **L_THRP_BITS_DL** columns).\n",
    "\n",
    "In the end you must get the following result:\n",
    "<img src=\"./images/time-series-output.png\">\n",
    "\n",
    "**Note:** To execute this exercise please use the dataframe obtained from the join section (**df_groupby_**)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Auxiliar Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start coding here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_series():\n",
    "    fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "    # line for PRB utilization\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=df_groupby_.index, y=df_groupby_['L_CHMEAS_PRB_DL_USED_AVG'], name=\"PRB Utilization\"),\n",
    "        secondary_y=False,\n",
    "    )\n",
    "\n",
    "    # line for throughput\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=df_groupby_.index, y=df_groupby_['L_THRP_BITS_DL'], name='Throughput'),\n",
    "        secondary_y=True,\n",
    "    )\n",
    "\n",
    "    # Set axes titles\n",
    "    fig.update_xaxes(title_text=\"Hours\")\n",
    "    fig.update_yaxes(title_text=\"<b>PRB Usage</b>\", secondary_y=False)\n",
    "    fig.update_yaxes(title_text=\"<b>Throughput</b>\", secondary_y=True)\n",
    "\n",
    "    # Add figure title\n",
    "    fig.update_layout(\n",
    "        title_text=\"Exercise 3 - Time Series\",\n",
    "        width=950\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "    \n",
    "# call the time-series function\n",
    "time_series()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3_4'></a>\n",
    "\n",
    "#### 3.4 Tables \n",
    "([Back to Index](#0))\n",
    "\n",
    "Tables are always handy in Exploratory Data Analysis procedures. \n",
    "\n",
    "In this section we want you to **create two TOP 5s** one for the cells with the higher **PRB Utilization** and the other for the cells with the smaller **Throughput** (which corresponds to the **L_CHMEAS_PRB_DL_USED_AVG** and **L_THRP_BITS_DL** columns, respectively). Both results must take into account the **DATETIME** column.\n",
    "\n",
    "In the end you must get the following result:\n",
    "\n",
    ">- **TOP 5 for PRB Utilization:**\n",
    "<img src=\"./images/top5-prb-output.png\">\n",
    "\n",
    ">- **TOP 5 for Throughput:**\n",
    "<img src=\"./images/top5-thrpt-output.png\">\n",
    "\n",
    "\n",
    "**Note:** To execute this exercise please use the dataframe obtained from the join section (**df_join_**)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Auxiliar Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_columns(df, cols):\n",
    "    '''\n",
    "    Function that drop columns that are in cols input\n",
    "    '''\n",
    "    \n",
    "    return df.drop([c for c in list(df.columns) if not c in cols], axis=1)\n",
    "\n",
    "def get_top5(df, col):\n",
    "    '''\n",
    "    Function that return top5 dataframe \n",
    "    '''\n",
    "    \n",
    "    return df.sort_values(by=col, ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top1_cols_ = ['DATETIME', 'LOCALCELL_ID', 'L_CHMEAS_PRB_DL_USED_AVG']\n",
    "top2_cols_ = ['DATETIME', 'LOCALCELL_ID', 'L_THRP_BITS_DL']\n",
    "\n",
    "df1_ = drop_columns(df_join_, top1_cols_)\n",
    "df2_ = drop_columns(df_join_, top2_cols_)\n",
    "\n",
    "df_top1_ = get_top5(df1_, top1_cols_[-1])\n",
    "df_top2_ = get_top5(df2_, top2_cols_[-1])\n",
    "\n",
    "\n",
    "def top5_table(df, cols, title):\n",
    "    '''\n",
    "    Function that outputs a top5 table\n",
    "    '''\n",
    "    \n",
    "    fig = go.Figure(data=[go.Table(\n",
    "    \n",
    "        header=dict(values=cols,\n",
    "                    fill_color='royalblue',\n",
    "                    align='left',\n",
    "                    font=dict(color='white', size=16),\n",
    "                    height=32\n",
    "                   ),\n",
    "        cells=dict(values=[df[cols[0]], df[cols[1]], df[cols[2]]],\n",
    "                   fill_color='lavender',\n",
    "                   align='left',\n",
    "                   font_size=15,\n",
    "                   height=30\n",
    "                  ))\n",
    "    ])\n",
    "\n",
    "    fig.update_layout(\n",
    "        height=370,\n",
    "        width=950,\n",
    "        title_text=title,\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# print the two top5s required\n",
    "top5_table(df_top1_, top1_cols_, 'Exercise 4 - TOP5 PRB Usage')\n",
    "top5_table(df_top2_, top2_cols_, 'Exercise 4 - TOP5 Throughput')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3_5'></a>\n",
    "\n",
    "#### 3.5 Panel Dashboards \n",
    "([Back to Index](#0))\n",
    "\n",
    "To collect visual elements into a Python Dashboard, use `panel`. \n",
    "\n",
    "In this section we want you to **create two dashboards with the elements that you have developed in the previous sections** (Pie Charts, Heat Map, Time Series Line Plot, and the TOPs).\n",
    "\n",
    "The first dashboard must have the title **\"Distributions\"** and must possess the Pie Charts and the Heat Map. The second one must have the title **Time-Series and Distributions** and must possess the TOPs (PRB and Throughput) and the Time Series Line Chart. \n",
    "\n",
    "In the end you must get the following result:\n",
    "\n",
    ">- **Dashboard 1:**\n",
    "<img src=\"./images/dashboard1-output.jpg\">\n",
    "\n",
    ">- **Dashboard 2:**\n",
    "<img src=\"./images/dashboard2-output.jpeg\">\n",
    "\n",
    "\n",
    "**Note:** To execute this exercise please use functions from the previous sections!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Auxiliar Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start coding here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a panel extension\n",
    "pn.extension('plotly')\n",
    "\n",
    "# dashboard title definition\n",
    "dashboard_title_ = '#### ** A&C Training - Visualization Fundamentals**'\n",
    "\n",
    "# tab1 - pie charts and time-series\n",
    "tab_1_ = pn.Column(pn.Row(pie_charts()),\n",
    "                   pn.Row(time_series()))\n",
    "\n",
    "# tab2 - heat map and top5\n",
    "tab_2_ = pn.Column(pn.Row(heat_map()),\n",
    "                   pn.Row(top5_table(df_top1_, top1_cols_, 'Exercise 4 - TOP5 PRB Usage')),\n",
    "                   pn.Row(top5_table(df_top2_, top2_cols_, 'Exercise 4 - TOP5 Throughput')))\n",
    "\n",
    "# define the dashboards\n",
    "dashboard_ = pn.Column(dashboard_title_, pn.Tabs(('1. Pie Charts & Time Series', tab_1_), ('2. Heat Map & TOPs', tab_2_)))\n",
    "\n",
    "dashboard_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
